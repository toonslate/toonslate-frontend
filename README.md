# Toonslate

"ToonSlate"는 웹툰의 한국어 텍스트를 영어로 바꿔주는 웹 서비스입니다.

<!-- TODO: 로고 또는 대표 스크린샷 -->

> 🔗 배포: https://www.toonslate.site

## 📖 Table of Contents

- [💡 Motivation](#-motivation)
- [🕹️ Features](#️-features)
- [🛠 Tech Stack](#-tech-stack)
  - [Frontend](#frontend)
  - [Backend](#backend)
  - [AI Pipeline](#ai-pipeline)
  - [Infra](#infra)
- [🏔 Challenges](#-challenges)
  - [1. AI 번역 파이프라인: 비용과 품질 사이의 설계](#1-ai-번역-파이프라인-비용과-품질-사이의-설계)
    - [1차 파이프라인: 특화 모델 조합](#1차-파이프라인-특화-모델-조합)
    - [2차 파이프라인: Gemini 원샷 전환](#2차-파이프라인-gemini-원샷-전환)
    - [3차 파이프라인: 다시 멀티 모델로](#3차-파이프라인-다시-멀티-모델로)
  - [2. Celery + FastAPI: 동시에 존재한 두 개의 버그](#2-celery--fastapi-동시에-존재한-두-개의-버그)
    - [증상: 태스크가 사라지는 문제](#증상-태스크가-사라지는-문제)
    - [문제 1: Async Redis + Celery Event Loop 충돌](#문제-1-async-redis--celery-event-loop-충돌)
    - [문제 2: Producer Connection Pool 버퍼링](#문제-2-producer-connection-pool-버퍼링)
  - [3. Canvas 기반 리터치 에디터](#3-canvas-기반-리터치-에디터)
    - [설계: 서버는 처리만, 프론트엔드가 편집 관리](#설계-서버는-처리만-프론트엔드가-편집-관리)
    - [좌표 밀림 문제: CSS 3줄로 근본 해결](#좌표-밀림-문제-css-3줄로-근본-해결)
- [📝 회고](#-회고)

---

## 💡 Motivation

웹툰 번역은 단순 텍스트 번역이 아닙니다. 이미지 속 텍스트를 찾고, 읽고, 번역하고, 원본을 지우고, 번역문을 다시 채워넣는 과정이 필요합니다. 이 과정을 수작업으로 하면 한 화에 상당한 시간이 걸립니다.

Toonslate는 각 단계에 특화된 AI 모델을 조합하여 이 과정을 자동화합니다. 하나의 비싼 범용 모델에 의존하는 대신, 단계별로 적절한 도구를 선택하여 비용을 줄이면서 문제를 해결하는 것이 목적입니다.

<!-- TODO: Preview (핵심 기능별 GIF/스크린샷) -->

---

## 🕹️ Features

- **📤 다중 이미지 번역**: 여러 장의 웹툰 이미지를 업로드하면 한 번에 번역 처리
- **🔄 자동 번역 파이프라인**: 텍스트 감지 → Inpainting → 번역 → 렌더링을 자동 수행
- **🖌️ 리터치 에디터**: 번역 결과가 마음에 들지 않는 부분을 브러시로 마킹하여 수정 (Undo/Redo 지원)

---

## 🛠 Tech Stack

### Frontend

| | |
|---|---|
| Core | React 19, TypeScript, Vite |
| Styling | Tailwind CSS v4, shadcn/ui |

- **TypeScript**: 프론트엔드-백엔드 간 좌표, 이미지 메타데이터 등 복잡한 데이터를 주고받는 구조에서 타입 안정성을 확보하고자 하였습니다.
- **shadcn/ui**: 라이브러리에 종속되지 않고 코드를 직접 소유하는 구조라, 필요 시 빠르게 수정이 가능하며 Tailwind CSS와 조합이 좋습니다.

<br>

### Backend

| | |
|---|---|
| Core | Python 3.11, FastAPI |
| Task Queue | Celery + Redis |
| Deploy | Docker Compose |

#### Why Python?

AI 모델 생태계(PyTorch, HuggingFace 등)와의 호환성을 고려했습니다. 모델 성능이 부족할 경우 직접 파인튜닝해야 할 수 있어, AI 친화적인 언어를 선택했습니다.

#### Why FastAPI?

| | Django | FastAPI |
|---|---|---|
| 비동기 지원 | 제한적 | 네이티브 (async/await) |
| API 문서화 | 별도 설정 필요 | Swagger 자동 생성 |
| 번들 크기 | ORM, 관리자 페이지 등 포함 | 필요한 것만 선택 |

API 서버 역할에 집중하는 가벼운 프레임워크가 필요했고, 비동기 지원과 자동 문서화가 결정적이었습니다.

#### Why Celery + Redis?

번역 파이프라인은 이미지당 AI API 호출이 여러 번 발생하며, 다중 이미지 요청 시 전체 처리 시간이 길어집니다. 요청을 받는 웹 서버가 블로킹되지 않도록 비동기 작업 큐로 분리했습니다.

<br>

### AI Pipeline

```
Detection(YOLO v8) → Inpainting(LaMa) → Translation(Gemini) → Rendering(Pillow)
```

| 단계 | 모델 | 역할 |
|------|------|------|
| Detection | YOLO v8 (만화 특화 학습 모델) | 말풍선과 텍스트 영역 감지 |
| Inpainting | LaMa | 원본 텍스트를 주변 배경으로 자연스럽게 제거 |
| Translation | Gemini Flash 2.5 Lite | 크롭된 텍스트 영역의 OCR + 번역 |
| Rendering | Pillow | 번역문을 이미지에 렌더링 |

- **Detection**: 최신 YOLO v11 대신, 만화 말풍선/텍스트에 특화된 학습 데이터로 훈련된 v8 모델을 선택했습니다.
- **Translation**: PaddleOCR 등 기존 OCR을 테스트했으나 한국어 인식과 비정형 텍스트 처리가 부족했습니다. Vision + 번역을 동시에 처리할 수 있는 Gemini 모델 중 가성비가 좋은 Flash Lite를 채택했습니다.
- **Inpainting**: Stable Diffusion 등 생성형 모델도 가능하지만 비용이 높아, 오픈소스 LaMa 모델을 선택했습니다.

<br>

### Infra

| | |
|---|---|
| Frontend | Vercel |
| Backend | AWS EC2 (t3.micro) + Docker Compose |
| SSL | nginx + Let's Encrypt |

Celery worker와 Redis가 같은 서버에서 실행되어야 하므로, 서버리스(Lambda) 대신 EC2에 docker-compose로 web + worker + redis를 함께 배포하는 구조를 선택했습니다.

---

## 🏔 Challenges

### 1. AI 번역 파이프라인: 비용과 품질 사이의 설계

#### 1차 파이프라인: 특화 모델 조합

초기 파이프라인은 각 단계에 특화된 모델을 조합하는 구조였습니다.

```
Detection(YOLO v8) → OCR(Gemini) → Translation(Gemini) → Inpainting(LaMa) → Rendering(Pillow)
```

만화 말풍선/텍스트 감지에 특화된 YOLO v8 모델로 텍스트 영역을 찾고, Gemini로 읽고 번역한 뒤, LaMa로 원본 텍스트를 지우고 번역문을 렌더링하는 방식이었습니다. 각 단계에 비용이 낮은 모델을 배치해 전체 비용을 줄이려는 의도였지만, 첫 단계인 Detection의 실효 정확도가 약 64~81%에 머물렀습니다.

한국 웹툰은 일본 만화보다 레이아웃이 비정형적이라 기존 학습 데이터로는 한계가 있었고, Detection이 놓친 텍스트는 이후 단계에서 복구할 방법이 없었습니다.

<br>

#### 2차 파이프라인: Gemini 원샷 전환

Detection 병목을 해결하기 위해 Gemini의 이미지 생성 모델(Nano Banana)로 전환했습니다. 이미지를 통째로 넘기면 텍스트를 찾아 번역까지 한 번에 처리해주는 원샷 방식이었습니다. 어차피 써야한다면 차라리 하나의 과정으로 통합해 문제를 쉽게 해결하자는 판단이었습니다.

처음에는 잘 해결되는 것으로 보였습니다. Detection 정확도 문제도 없어보였고, API 호출도 이미지당 1회로 감소했습니다.

하지만 두 가지 문제가 드러났습니다.

- **💰 비용**: Pro 모델 기준 이미지당 약 $0.134. 웹툰 1화(10장)에 약 1,940원, 월 3,000건이면 약 58만 원
- **🎯 SFX(효과음) 정확도**: 말풍선 텍스트는 잘 처리했지만, 비정형적인 효과음까지 완전하게 번역되는 경우는 약 20~30%에 불과

또한 Gemini API가 입력과 다른 해상도로 출력하는 문제도 발견했습니다. 출력이 약 1백만 픽셀 근처로 정규화되어, 1200x1882px 입력이 816x1296px로 축소되는 식이었습니다.

<br>

#### 3차 파이프라인: 다시 멀티 모델로

원샷의 편리함에 혹했지만, 비용과 제어 가능성 문제를 해결할 수 없었습니다. 결국 1차 파이프라인의 방향, 즉 특화된 모델과 알고리즘의 조합으로 돌아가기로 했습니다. 다만 현실적으로 가능한 부분과 개선 가능한 사안에 집중하고자 하였습니다.

```
Detection(YOLO v8) → Inpainting(LaMa/단색 채우기) → Translation(Gemini) → Rendering(Pillow)
```

- **Detection**: 기존 YOLO v8 유지하되, 말풍선 텍스트에 집중 (SFX는 best-effort)
- **Inpainting**: 말풍선(단색 배경)은 단순 색 채우기, 복잡한 배경은 LaMa로 분기
- **Translation**: Gemini를 크롭 이미지 번역에만 사용하여 비용 절감 (전체 이미지 → 텍스트 영역만)
- 사용자 리터치 에디터로 미처리 영역을 보완

이 프로젝트의 애초 취지는 "AI 시대에 특화 모델과 알고리즘을 조합해서 비용을 줄이면서 문제를 해결하자"였습니다. 원샷 방식은 그 취지에서 벗어난 선택이었고, 결국 돌아오게 되었습니다. 이 과정에서 개발 시간을 많이 소모했지만, AI 도구 선택에서 비용/품질/제어 가능성의 트레이드오프를 체감하는 계기가 되었습니다.

---

### 2. Celery + FastAPI: 동시에 존재한 두 개의 버그

#### 증상: 태스크가 사라지는 문제

번역 요청을 보내면 FastAPI 로그에는 "태스크 dispatch 완료"라고 나오지만, Celery worker는 아무런 작업도 받지 못했습니다. Redis 큐를 직접 확인해도 비어있었고, 작업 상태는 계속 `pending`이었습니다. 파이프라인 전체가 작동하지 않는 치명적인 문제였습니다.

<br>

#### 문제 1: Async Redis + Celery Event Loop 충돌

Celery worker가 태스크를 처리하려 하면 `RuntimeError: Event loop is closed` 에러가 발생했습니다.

원인은 FastAPI와 Celery의 이벤트 루프 충돌이었습니다. FastAPI는 async 컨텍스트에서 실행되고, `redis.asyncio.Redis` 클라이언트는 특정 이벤트 루프에 바인딩됩니다. 그런데 Celery worker는 자체적으로 새 이벤트 루프를 생성하기 때문에, 기존 루프에 연결된 async Redis 연결이 새 루프에서 작동하지 않았습니다.

해결은 동기 Redis 클라이언트로 전환하는 것이었습니다. Redis 호출은 약 0.001초 수준으로 병목이 아니었고, 실제 병목은 AI API 호출(10~15초)이었기 때문에 async의 이점이 없었습니다.

<br>

#### 문제 2: Producer Connection Pool 버퍼링

첫 번째 문제를 해결해도 태스크는 여전히 전달되지 않았습니다. 두 번째 문제가 숨어있었습니다.

디버깅 과정에서 결정적인 단서를 찾았습니다. CLI에서 수동으로 `process_job.delay("test")`를 호출하면 Celery가 정상적으로 받았지만, FastAPI에서 같은 코드를 실행하면 받지 못했습니다. 차이점은 CLI는 프로세스 종료 시 연결이 정리되며 메시지가 flush되지만, FastAPI는 계속 실행 중이라 연결 풀의 버퍼에 메시지가 남아있었다는 것이었습니다.

```python
# 문제: delay()가 AsyncResult를 반환해도 메시지가 실제로 전송되지 않음
process_job.delay(job_id)

# 해결: 명시적 연결 관리로 즉시 flush
with process_job.app.producer_pool.acquire(block=True) as producer:
    process_job.apply_async(args=[job_id], producer=producer)
```

`with` 블록이 끝나면 producer가 풀에 반환되면서 버퍼가 flush되어 메시지가 Redis로 즉시 전송됩니다.

<br>

#### 교훈

이 경험에서 가장 크게 배운 것은 **두 개의 별개 문제가 동시에 존재할 수 있다**는 것이었습니다. 첫 번째 문제(async Redis)를 해결했을 때 "이제 되겠지"라고 생각했지만, 두 번째 문제(producer pool)가 남아있었습니다.

또한 CLI에서 성공한 테스트가 서버 환경에서 실패할 수 있다는 것, 그리고 "dispatch 성공"이 "전송 완료"를 의미하지 않는다는 것을 배웠습니다.

---

### 3. Canvas 기반 리터치 에디터

#### 설계: 서버는 처리만, 프론트엔드가 편집 관리

번역 결과에서 지워야 할 부분이 있을 때, 사용자가 브러시로 영역을 마킹하면 서버의 LaMa 모델이 해당 영역을 주변 배경으로 자연스럽게 채워주는 Erase 기능을 구현했습니다. 저장 방식을 결정할 때 네 가지 선택지를 검토했습니다.

| 방식 | 설명 | 채택 여부 |
|------|------|----------|
| 서버 버전 관리 | 매 수정마다 새 파일 저장 | 서버 복잡도 높음 |
| 덮어쓰기 | 항상 최신만 저장 | Undo 불가 |
| **서버는 반환만, FE가 관리** | 서버는 이미지 처리 API일 뿐 | **채택** |
| 하이브리드 | 위 + "저장" 버튼 | 불필요한 복잡도 |

서버는 "이미지 + 마스크를 받아 처리된 이미지를 반환"하는 단순한 API로 유지하고, 편집 히스토리는 프론트엔드 Canvas 메모리에서 관리하기로 했습니다.

Undo/Redo는 스냅샷 기반으로 구현했습니다. Erase는 서버 API를 거치는 비가역적 작업이라 액션 기반 역연산이 불가능했기 때문에, 매 액션 후 Canvas 전체를 base64 PNG로 저장하는 방식을 택했습니다. 800x2000px 기준으로 스냅샷 하나가 약 6.4MB, 10단계 제한으로 최대 약 64MB 수준이라 브라우저 한계(1~2GB) 내에서 충분했습니다.

<br>

#### 좌표 밀림 문제: CSS 3줄로 근본 해결

리터치 모드에서 브러시 마킹 좌표가 실제 의도한 위치보다 왼쪽으로 밀리는 문제가 발생했습니다. 결과 이미지를 표시하는 캔버스와 마스크를 그리는 캔버스의 CSS 크기가 달랐기 때문이었습니다. 결과 캔버스는 `max-w-full`로 이미지 원본 크기까지만 차지하고, 마스크 캔버스는 `width: 100%`로 부모 전체를 채우고 있었습니다.

네 가지 해결법을 검토했습니다.

| 방법 | 변경 규모 | 근본 해결 |
|------|----------|----------|
| **CSS-only** | 3줄 | O |
| 좌표 보정 | ~10줄 | △ (증상 치료) |
| ResizeObserver | ~15줄 | O |
| 단일 캔버스 | 대규모 | O |

CSS-only 접근을 선택했습니다. 부모 div에 `w-fit`을 추가해 결과 캔버스 크기에 맞추고, 마스크 캔버스의 인라인 style을 제거하고 `inset-0`으로 통일했습니다. 3줄의 CSS 변경으로 근본 원인을 해결했고, 줌/팬 같은 기능이 필요해지면 그때 ResizeObserver나 좌표계 분리로 확장하면 된다고 판단했습니다.

---

## 📝 회고

### 쉬운 길의 유혹

이 프로젝트의 출발점은 "AI 시대에 특화 모델과 알고리즘을 조합해서, 비용을 줄이면서 문제를 해결하자"였습니다. 개발자의 본질적인 역할은 문제를 해결하는 것이고, 더 정밀하게 말하면 문제를 **더 값싸게** 해결하는 것이라고 생각했습니다.

하지만 1차 파이프라인에서 Detection 품질이 기대에 못 미치자, "더 비싸지만 더 좋은 모델(Gemini 원샷)을 돈 주고 쓰자"는 쉬운 길에 혹했습니다. 원샷 방식은 확실히 편했지만, 비용이 높았고, SFX 정확도는 20~30%에 불과했으며, 해상도 제어도 불가능했습니다. 결국 처음의 방향, 즉 특화 모델 조합으로 돌아오게 되었고, 그 과정에서 개발 시간을 상당히 소모했습니다.

돌이켜보면 1차 파이프라인의 Detection 문제를 만났을 때, 모델을 통째로 바꾸는 것이 아니라 Detection 단계 자체를 개선하거나 우회하는 방향을 먼저 탐색했어야 했습니다. AI를 활용한 프로젝트에서 "성능이 안 나오면 더 좋은 모델로"라는 접근은 결국 비용 문제로 돌아온다는 것을 체감했습니다.
